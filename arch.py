from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import os, warnings, requests
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import io
import tensorflow as tf
from tensorflow.keras import layers, models

"""
V02 Inference API â€” è¶³å¼“åˆ†ææœå‹™ï¼ˆæ•´åˆæ­¥æ…‹äº‹ä»¶é æ¸¬ï¼‰
==================================================
- æ¥æ”¶ Swift App ä¸Šå‚³çš„ CSV æª”æ¡ˆ
- å…ˆå‘¼å«ç¬¬äºŒä»½æœå‹™é€²è¡Œæ­¥æ…‹äº‹ä»¶é æ¸¬ï¼ˆHS/TOï¼‰
- ä½¿ç”¨é æ¸¬çµæœé€²è¡Œè¶³å¼“åˆ†æ
- è¼¸å‡ºè¶³å¼“é¡å‹é æ¸¬çµ¦ Swift App
"""

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --------------------------- é…ç½®åƒæ•¸ ---------------------------
GAIT_PREDICTION_URL = "http://localhost:8000/predict/"  # ç¬¬äºŒä»½æœå‹™çš„ URL
PAA_IDX = 100
WIN_STEPS = 5
STRIDE_PAA_WIN = 2
FEAT_COLS = ["Gyroscope_X","Gyroscope_Y","Gyroscope_Z",
             "Acceleration_X","Acceleration_Y","Acceleration_Z"]

# --------------------------- æ¨¡å‹åŠ è¼‰ ---------------------------
def load_arch_model():
    """åŠ è¼‰è¶³å¼“åˆ†ææ¨¡å‹"""
    try:
        model_path = r"C:\GITHUB\GaitHelper\weight\V02_Infer.keras"
        model = tf.keras.models.load_model(model_path, compile=False)
        print("âœ… è¶³å¼“åˆ†ææ¨¡å‹åŠ è¼‰æˆåŠŸ")
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        return None

# å…¨å±€æ¨¡å‹è®Šé‡
arch_model = load_arch_model()

# --------------------------- è¼”åŠ©å‡½æ•¸ ---------------------------
def call_gait_prediction_service(csv_data: bytes, filename: str) -> pd.DataFrame:
    """
    å‘¼å«ç¬¬äºŒä»½æ­¥æ…‹äº‹ä»¶é æ¸¬æœå‹™
    è¿”å›æ·»åŠ äº† Pred_result æ¬„ä½çš„ DataFrame
    """
    try:
        # æº–å‚™ä¸Šå‚³æª”æ¡ˆ
        files = {'file': (filename, csv_data, 'text/csv')}
        
        print(f"ğŸ“¡ å‘¼å«æ­¥æ…‹äº‹ä»¶é æ¸¬æœå‹™: {GAIT_PREDICTION_URL}")
        response = requests.post(GAIT_PREDICTION_URL, files=files, timeout=30)
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"æ­¥æ…‹é æ¸¬æœå‹™éŒ¯èª¤: {response.text}"
            )
        
        result = response.json()
        
        if result.get("status") != "success":
            raise HTTPException(
                status_code=500,
                detail=f"æ­¥æ…‹é æ¸¬å¤±æ•—: {result.get('error', 'Unknown error')}"
            )
        
        # è®€å–åŸå§‹ CSV
        df = pd.read_csv(io.BytesIO(csv_data))
        
        # æ·»åŠ  Pred_result æ¬„ä½åŸºæ–¼é æ¸¬çµæœ
        predictions = result.get("predictions", [])
        df["Pred_result"] = ""  # åˆå§‹åŒ–ç‚ºç©ºå­—ä¸²
        
        # å°‡é æ¸¬äº‹ä»¶æ˜ å°„åˆ°å°æ‡‰çš„æ™‚é–“é»
        time_values = df["Time"].values if "Time" in df.columns else df.index.values
        
        for pred in predictions:
            pred_time = pred.get("time")
            pred_event = pred.get("event")
            
            if pred_time is not None and pred_event in ["HS", "TO"]:
                # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ™‚é–“é»ç´¢å¼•
                time_diff = np.abs(time_values - pred_time)
                closest_idx = np.argmin(time_diff)
                
                # æª¢æŸ¥æ™‚é–“å·®ç•°æ˜¯å¦åœ¨åˆç†ç¯„åœå…§
                if time_diff[closest_idx] < 0.1:  # 100ms å®¹éŒ¯
                    df.at[closest_idx, "Pred_result"] = pred_event
        
        print(f"âœ… æ­¥æ…‹äº‹ä»¶é æ¸¬å®Œæˆï¼Œæª¢æ¸¬åˆ° {len(predictions)} å€‹äº‹ä»¶")
        return df
        
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=503,
            detail=f"ç„¡æ³•é€£æ¥æ­¥æ…‹é æ¸¬æœå‹™: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ­¥æ…‹é æ¸¬è™•ç†éŒ¯èª¤: {str(e)}"
        )

def paa_fast(seg, M=100):
    """PAA å£“ç¸®"""
    L, F = seg.shape
    idx = (np.linspace(0, L, M+1)).astype(int)
    out = np.add.reduceat(seg, idx[:-1], axis=0)
    w   = np.maximum(np.diff(idx)[:, None], 1)
    return out / w

def five_step_windows(paa_list, win=5, stride=STRIDE_PAA_WIN):
    """ç”Ÿæˆ5æ­¥è¦–çª—"""
    n = len(paa_list)
    if n < win:
        return np.empty((0, win*PAA_IDX, len(FEAT_COLS)), dtype=np.float32)
    seqs = []
    for s in range(0, n - win + 1, stride):
        seq = np.concatenate(paa_list[s:s+win], axis=0)
        seqs.append(seq.astype(np.float32))
    return np.stack(seqs, axis=0) if seqs else np.empty((0, win*PAA_IDX, len(FEAT_COLS)), dtype=np.float32)

def process_csv_for_arch_prediction(df):
    """
    ä½¿ç”¨æ­¥æ…‹äº‹ä»¶é æ¸¬çµæœé€²è¡Œè¶³å¼“åˆ†æ
    """
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    missing_feat = [col for col in FEAT_COLS if col not in df.columns]
    if missing_feat:
        raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_feat}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ TO äº‹ä»¶
    if "Pred_result" not in df.columns:
        raise ValueError("ç¼ºå°‘ Pred_result æ¬„ä½ï¼Œè«‹å…ˆé€²è¡Œæ­¥æ…‹äº‹ä»¶é æ¸¬")
    
    # TOâ†’TO åˆ‡æ­¥
    ev = df["Pred_result"].astype(str).str.upper().fillna("")
    to_idx = df.index[ev.eq("TO")].to_numpy()
    
    print(f"ğŸ” æª¢æ¸¬åˆ° {len(to_idx)} å€‹ TO äº‹ä»¶")
    
    paa_list = []
    step_spans = []
    sensor_data = df[FEAT_COLS].to_numpy(dtype=np.float32)

    for i in range(len(to_idx) - 1):
        a, b = int(to_idx[i]), int(to_idx[i+1])
        if b <= a:
            continue
        step = sensor_data[a:b]
        
        # ç¢ºä¿æ­¥é•·è¶³å¤ é€²è¡Œ PAA
        if len(step) < 10:  # æœ€å°æ­¥é•·é™åˆ¶
            continue
            
        paa = paa_fast(step, PAA_IDX)
        paa_list.append(paa)
        step_spans.append((a, b))

    # ç”Ÿæˆ5æ­¥è¦–çª—
    X_seq = five_step_windows(paa_list, win=WIN_STEPS, stride=STRIDE_PAA_WIN)
    
    return X_seq, len(paa_list), len(X_seq), len(to_idx)

# --------------------------- API ç«¯é» ---------------------------
@app.post("/analyze-arch/")
async def analyze_arch(file: UploadFile = File(...)):
    """
    è¶³å¼“åˆ†æç«¯é»ï¼ˆæ•´åˆæ­¥æ…‹äº‹ä»¶é æ¸¬ï¼‰
    """
    try:
        if arch_model is None:
            return JSONResponse(
                status_code=500, 
                content={"error": "è¶³å¼“åˆ†ææ¨¡å‹æœªåŠ è¼‰ï¼Œæœå‹™ä¸å¯ç”¨"}
            )
        
        # è®€å–ä¸Šå‚³çš„ CSV
        contents = await file.read()
        
        print(f"ğŸ“Š æ¥æ”¶æª”æ¡ˆ: {file.filename}, å¤§å°: {len(contents)} bytes")
        
        # æ­¥é©Ÿ1: å‘¼å«æ­¥æ…‹äº‹ä»¶é æ¸¬æœå‹™
        df_with_events = call_gait_prediction_service(contents, file.filename)
        
        # æ­¥é©Ÿ2: é€²è¡Œè¶³å¼“åˆ†æ
        X_seq, n_steps, n_windows, n_to_events = process_csv_for_arch_prediction(df_with_events)
        
        if n_windows == 0:
            return JSONResponse(
                status_code=400, 
                content={
                    "error": "ç„¡æ³•ç”Ÿæˆè¶³å¤ çš„åˆ†æè¦–çª—",
                    "details": {
                        "to_events_detected": n_to_events,
                        "steps_formed": n_steps,
                        "windows_generated": n_windows
                    }
                }
            )
        
        print(f"ğŸ” æ­¥æ…‹åˆ†æ: {n_to_events} TOäº‹ä»¶ â†’ {n_steps}æ­¥ â†’ {n_windows}è¦–çª—")
        
        # æ­¥é©Ÿ3: æ¨¡å‹é æ¸¬
        probs = arch_model.predict(X_seq, batch_size=256, verbose=0)
        
        # è¨ˆç®—æ•´é«”é æ¸¬çµæœ
        flat_prob_mean = float(probs[:, 1].mean())
        normal_prob_mean = float(probs[:, 0].mean())
        
        # å¤šæ•¸æ±º
        preds = probs.argmax(axis=1)
        final_prediction = int(np.median(preds))
        
        arch_type = "flat" if final_prediction == 1 else "normal"
        confidence = flat_prob_mean if final_prediction == 1 else normal_prob_mean
        
        # æ§‹å»ºéŸ¿æ‡‰
        result = {
            "status": "success",
            "gait_analysis": {
                "to_events_detected": n_to_events,
                "steps_formed": n_steps,
                "windows_analyzed": n_windows
            },
            "arch_analysis": {
                "arch_type": arch_type,
                "confidence": round(confidence, 4),
                "normal_probability": round(normal_prob_mean, 4),
                "flat_probability": round(flat_prob_mean, 4),
                "prediction_code": final_prediction
            },
            "details": {
                "total_data_points": len(df_with_events),
                "processing_steps": "æ­¥æ…‹äº‹ä»¶é æ¸¬ â†’ TOåˆ‡æ­¥ â†’ PAAå£“ç¸® â†’ 5æ­¥è¦–çª— â†’ è¶³å¼“åˆ†é¡"
            }
        }
        
        print(f"âœ… è¶³å¼“åˆ†æå®Œæˆ: {arch_type} (ç½®ä¿¡åº¦: {confidence:.3f})")
        return result
        
    except HTTPException as e:
        print(f"âŒ HTTPéŒ¯èª¤: {e.detail}")
        return JSONResponse(
            status_code=e.status_code,
            content={"error": e.detail}
        )
    except Exception as e:
        print(f"âŒ åˆ†æéŒ¯èª¤: {str(e)}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"è¶³å¼“åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}
        )

@app.post("/analyze-arch-direct/")
async def analyze_arch_direct(file: UploadFile = File(...)):
    """
    ç›´æ¥è¶³å¼“åˆ†æç«¯é»ï¼ˆä¸ä½¿ç”¨æ­¥æ…‹äº‹ä»¶é æ¸¬ï¼Œç”¨æ–¼æ¸¬è©¦ï¼‰
    """
    try:
        if arch_model is None:
            return JSONResponse(
                status_code=500, 
                content={"error": "è¶³å¼“åˆ†ææ¨¡å‹æœªåŠ è¼‰ï¼Œæœå‹™ä¸å¯ç”¨"}
            )
        
        contents = await file.read()
        df = pd.read_csv(io.BytesIO(contents))
        
        # ä½¿ç”¨è‡ªå‹• TO æª¢æ¸¬ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        from scipy.signal import find_peaks
        
        if "Gyroscope_Z" in df.columns:
            gyro_z = df["Gyroscope_Z"].values
            to_indices = find_peaks(-gyro_z, height=2.0, distance=30)[0]
            df["Pred_result"] = ""
            df.loc[to_indices, "Pred_result"] = "TO"
        
        X_seq, n_steps, n_windows, n_to_events = process_csv_for_arch_prediction(df)
        
        if n_windows == 0:
            return JSONResponse(
                status_code=400, 
                content={"error": "ç„¡æ³•ç”Ÿæˆè¶³å¤ çš„åˆ†æè¦–çª—"}
            )
        
        probs = arch_model.predict(X_seq, batch_size=256, verbose=0)
        flat_prob_mean = float(probs[:, 1].mean())
        normal_prob_mean = float(probs[:, 0].mean())
        final_prediction = int(np.median(probs.argmax(axis=1)))
        
        arch_type = "flat" if final_prediction == 1 else "normal"
        confidence = flat_prob_mean if final_prediction == 1 else normal_prob_mean
        
        return {
            "status": "success",
            "arch_type": arch_type,
            "confidence": round(confidence, 4),
            "normal_probability": round(normal_prob_mean, 4),
            "flat_probability": round(flat_prob_mean, 4),
            "steps_detected": n_steps,
            "windows_analyzed": n_windows,
            "method": "direct_auto_to_detection"
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500, 
            content={"error": f"ç›´æ¥åˆ†æéŒ¯èª¤: {str(e)}"}
        )

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    gait_service_available = False
    try:
        response = requests.get(GAIT_PREDICTION_URL.replace("/predict/", "/"), timeout=5)
        gait_service_available = response.status_code == 200
    except:
        pass
    
    return {
        "status": "healthy",
        "arch_model_loaded": arch_model is not None,
        "gait_prediction_service_available": gait_service_available,
        "service": "gait_arch_analysis_integrated"
    }

# --------------------------- å•Ÿå‹• ---------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…è¡çª